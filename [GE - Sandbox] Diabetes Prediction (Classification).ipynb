{"cells":[{"cell_type":"markdown","metadata":{"id":"RF_4-z8AACcb"},"source":["# Connect G-Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hzq5HOY1ABYz"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BljaOLNx-PEp"},"outputs":[],"source":["import os\n","default_dir = \"/content/drive/MyDrive/...\"\n","os.chdir(default_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"khXENvONAWaR"},"outputs":[],"source":["!ls"]},{"cell_type":"markdown","metadata":{"id":"NauGBwdzZIqp"},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUiUJUU3ZJR6"},"outputs":[],"source":["import warnings\n","warnings.simplefilter(action=\"ignore\")\n","\n","import pandas as pd\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","from lightgbm import LGBMClassifier\n","from xgboost import XGBClassifier\n","\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","\n","from sklearn.ensemble import (\n","    GradientBoostingClassifier,\n","    RandomForestClassifier\n",")\n","\n","from sklearn.metrics import (\n","    accuracy_score,\n","    roc_auc_score,\n","    roc_curve,\n","    classification_report\n",")\n","\n","from sklearn.model_selection import (\n","    KFold,\n","    train_test_split,\n","    GridSearchCV,\n","    cross_val_score\n",")\n","\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"j1rXbVT-Aqj6"},"source":["# 1.&nbsp;Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuXCkg6es7Yc"},"outputs":[],"source":["df = pd.read_csv(\"diabetes.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2YZzGqaJu_sj"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"xa0zpTqypH8S"},"source":["## 1.1 Pima Indians Diabetes Database\n","\n","This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, **all patients here are females at least 21 years old of Pima Indian heritage**.\n","\n","The datasets consists of several medical predictor variables and one target variable, **Outcome**. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n","\n","We build a **machine learning model** to accurately predict whether or not the patients in the dataset have **diabetes or not.**\n","\n","- **Pregnancies**: Number of times pregnant\n","- **Glucose**: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n","- **BloodPressure**: Diastolic blood pressure (mm Hg)\n","- **SkinThickness**: Triceps skin fold thickness (mm)\n","- **Insulin**: 2-Hour serum insulin (mu U/ml)\n","- **BMI**: Body mass index (weight in kg/(height in m)^2)\n","- **DiabetesPedigreeFunction**: Diabetes pedigree function\n","- **Age**: Age (years)\n","- **Outcome**: Class variable (0 or 1) 268 of 768 are 1, the others are 0"]},{"cell_type":"markdown","metadata":{"id":"qvLses-GpH8W"},"source":["## 1.2 General Information on Variables"]},{"cell_type":"markdown","metadata":{"id":"99QyPYrApH8W"},"source":["### a. Glucose Tolerance Test\n","It is a blood test that involves taking multiple blood samples over time, usually 2 hours.It used to diagnose diabetes. The results can be classified as normal, impaired, or abnormal.\n","* **Normal Results for Diabetes ->** Two-hour glucose level less than 140 mg/dL\n","\n","* **Impaired Results for Diabetes ->** Two-hour glucose level 140 to 200 mg/dL\n","\n","* **Abnormal (Diagnostic) Results for Diabetes ->** Two-hour glucose level greater than 200 mg/dL\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x07-j44kpH8X"},"source":["### b. BloodPressure\n","The diastolic reading, or the bottom number, is the pressure in the arteries when the heart rests between beats. This is the time when the heart fills with blood and gets oxygen. A normal diastolic blood pressure is lower than 80. A reading of 90 or higher means you have high blood pressure.\n","\n","* **Normal**: Systolic below 120 and diastolic below 80\n","* **Elevated**: Systolic 120–129 and diastolic under 80\n","* **Hypertension stage 1**: Systolic 130–139 and diastolic 80–89\n","* **Hypertension stage 2**: Systolic 140-plus and diastolic 90 or more\n","* **Hypertensive crisis**: Systolic higher than 180 and diastolic above 120."]},{"cell_type":"markdown","metadata":{"id":"KESO_uonpH8X"},"source":["### c. BMI\n","\n","The standard weight status categories associated with BMI ranges for adults are shown in the following table.\n","\n","* Below 18.5 -> **Underweight**\n","* 18.5 – 24.9 -> **Normal or Healthy Weight**\n","* 25.0 – 29.9 -> **Overweight**\n","* 30.0 and Above -> **Obese**"]},{"cell_type":"markdown","metadata":{"id":"KLt1gDZgpH8Y"},"source":["### d. Triceps Skinfolds\n","For adults, the standard normal values for triceps skinfolds are:\n","* 18.0mm (women)"]},{"cell_type":"markdown","metadata":{"id":"JJW1meAFpH8a"},"source":["# 2.&nbsp;Exploratory Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BIj-WCO7pH8a","trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQsRqtGUpH8b","trusted":true},"outputs":[],"source":["df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPUzyob6pH8b","trusted":true},"outputs":[],"source":["df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"q_wmrHYvpH8b","trusted":true},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tCWoqMPipH8b","trusted":true},"outputs":[],"source":["# Getting various summary statistics\n","# There is notably a large difference between 99% and max values of predictors\n","# “Insulin”, ”SkinThickness”, ”DiabetesPedigreeFunction”\n","# There are extreme values-Outliers in our data set\n","\n","# See BMI Min: 0\n","df.describe(\n","    percentiles=[0.05, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]\n",").T"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tw5HJIvIpH8b","trusted":true},"outputs":[],"source":["# Target Variable: Categorical\n","df['Outcome'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ImpW2qtQpH8b","trusted":true},"outputs":[],"source":["df['Outcome'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGIc5vnpHrpP"},"outputs":[],"source":["df['Outcome'].value_counts(normalize=True)"]},{"cell_type":"markdown","metadata":{"id":"9cUwWLLHpH8c"},"source":["# 3.&nbsp;Data Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxbZrRXbpH8c","trusted":true},"outputs":[],"source":["plt.figure(figsize=(8, 6))\n","sns.heatmap(\n","    df.corr(),\n","    cmap='Blues',\n","    annot=True\n",");"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w8G6JU7bIIjP"},"outputs":[],"source":["df.nlargest(10, 'BloodPressure')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LS0xx9kSrFF"},"outputs":[],"source":["# df.corr()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmpPwxAmS2Rv"},"outputs":[],"source":["k = 10\n","k_largest_corr = df.corr().nlargest(k, 'Outcome')\n","k_largest_feats = k_largest_corr['Outcome'].index\n","list(k_largest_feats)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lFRN25AnpH8c","trusted":true},"outputs":[],"source":["# Outcome correlation matrix\n","\n","k = 9 # number of variables for heatmap\n","cols = df.corr().nlargest(k, 'Outcome')['Outcome'].index\n","corr_mat = df[cols].corr()\n","\n","# Visualize\n","plt.figure(figsize=(10, 5))\n","sns.heatmap(\n","    corr_mat, cmap='viridis', annot=True,\n",");"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QW05NPrCJTUg"},"outputs":[],"source":["# df.loc[df.Pregnancies==12]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53ESPdeZpH8c","trusted":true},"outputs":[],"source":["# see how the data is distributed.\n","df.hist(figsize=(20,20));"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpw8XaakXvik"},"outputs":[],"source":["df['Age'].mean(), df['Age'].median()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJT2MFTJpH8c","trusted":true},"outputs":[],"source":["for col in df.columns:\n","    if col != \"Outcome\":\n","        sns.catplot(\n","            data=df, x=\"Outcome\",\n","            y=col, hue=\"Outcome\")\n","        plt.grid()"]},{"cell_type":"markdown","metadata":{"id":"9jmyjtbHpH8c"},"source":["# 4.&nbsp;Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5F2L9TWPpH8c","trusted":true},"outputs":[],"source":["# Observation units for variables with a minimum value of zero are NaN,\n","# except for the pregnancy variable.\n","df.describe(\n","    percentiles=[0.05, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]\n",").T"]},{"cell_type":"markdown","metadata":{"id":"PJzrziClXMwc"},"source":["## Handling Missing Values: Imputation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUv6_CyTpH8c","trusted":true},"outputs":[],"source":["# NaN values of 0 for Glucose, Blood Pressure, Skin Thickness, Insulin, BMI\n","# We can write Nan instead of 0\n","\n","cols = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n","for col in cols:\n","    df[col].replace(0, np.NaN, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2Mq-kLVpH8c","trusted":true},"outputs":[],"source":["# now we can see missing values\n","df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awRIitDeYvYW"},"outputs":[],"source":["df[\"Outcome\"] == 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xxne2swpH8c","trusted":true},"outputs":[],"source":["# We can fill in NaN values with a median\n","# according to the target value\n","\n","cols = [\n","    \"Glucose\",\n","    \"BloodPressure\",\n","    \"SkinThickness\",\n","    \"Insulin\",\n","    \"BMI\"\n","]\n","\n","mask_label_zero = (df[\"Outcome\"] == 0)\n","mask_label_one = (df[\"Outcome\"] == 1)\n","\n","for col in cols:\n","\n","    mask_col_null = df[col].isnull()\n","    col_median_zero = df[mask_label_zero][col].median()\n","    col_median_one = df[mask_label_one][col].median()\n","\n","    df.loc[(mask_label_zero & mask_col_null), col] = col_median_zero\n","    df.loc[(mask_label_one & mask_col_null), col] = col_median_one"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8IH1GpC6pH8d","trusted":true},"outputs":[],"source":["df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p7RHcbkqgG6Y"},"outputs":[],"source":["# df[(df['Pregnancies'] == 1)].any(axis=0)\n","# df[(df['Pregnancies'] == 1)].any(axis=None)\n","# (df['Pregnancies'] == 1).all(axis=None)"]},{"cell_type":"markdown","metadata":{"id":"HTswH8DGX_pl"},"source":["## Outlier Handling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bk5GnbkopH8d","trusted":true},"outputs":[],"source":["def outlier_thresholds(\n","        df, feature,\n","        quantile_lower=0.25,\n","        quantile_upper=0.75):\n","\n","    Q1 = df[feature].quantile(quantile_lower)\n","    Q3 = df[feature].quantile(quantile_upper)\n","    IQR = Q3 - Q1\n","\n","    lower_limit = Q1 - 1.5 * IQR\n","    upper_limit = Q3 + 1.5 * IQR\n","\n","    return lower_limit, upper_limit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVkU2SzPpH8d","trusted":true},"outputs":[],"source":["def has_outliers(\n","        df, feature,\n","        quantile_lower=0.25,\n","        quantile_upper=0.75):\n","    \"\"\"\n","    Args:\n","        df (pd.DataFrame): DataFrame containing feature\n","        feature (str): feature name to be checked\n","\n","    Return:\n","        bool: Is outlier(s) exist in given feature in the DataFrame\n","    \"\"\"\n","    low_lim, up_lim = outlier_thresholds(\n","        df, feature, quantile_lower, quantile_upper)\n","    exist_lower_outliers = (df[feature] < low_lim).any(axis=None)\n","    exist_upper_outliers = (df[feature] > up_lim).any(axis=None)\n","\n","    return (exist_lower_outliers or exist_upper_outliers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JYYdvE5EpH8d","trusted":true},"outputs":[],"source":["for feat in df.columns:\n","    exist_outliers = has_outliers(\n","        df, feat, quantile_lower=0.1, quantile_upper=0.9)\n","    if exist_outliers:\n","        print(f\"Outliers exist in {feat}!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEu9YpVLWTz-"},"outputs":[],"source":["feats_with_outliers = []\n","\n","for feat in df.columns:\n","    exist_outliers = has_outliers(\n","        df, feat, quantile_lower=0.1, quantile_upper=0.9)\n","    if exist_outliers:\n","        feats_with_outliers.append(feat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EmiSbkrxWyiM"},"outputs":[],"source":["feats_with_outliers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imfEO7KgpH8d","trusted":true},"outputs":[],"source":["def replace_with_thresholds(df, numerical_feats):\n","    for feat in numerical_feats:\n","        low_limit, up_limit = outlier_thresholds(df, feat)\n","\n","        mask_lower = (df[feat] < low_limit)\n","        mask_upper = (df[feat] > up_limit)\n","\n","        df.loc[mask_lower, feat] = low_limit\n","        df.loc[mask_upper, feat] = up_limit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXMwAIcWpH8d","trusted":true},"outputs":[],"source":["replace_with_thresholds(df,feats_with_outliers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nxMQkwzWpH8d","trusted":true},"outputs":[],"source":["df.describe(\n","    percentiles=[0.05, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]).T"]},{"cell_type":"markdown","metadata":{"id":"QhASSSVppH8d"},"source":["# 5.&nbsp;Feature Engineering\n","\n","See 1.1 & 1.2"]},{"cell_type":"markdown","metadata":{"id":"m1P-7nAgvB_V"},"source":["## Feature Categorization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJ5kLb_mgiwP"},"outputs":[],"source":["max(df['Glucose'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2m4eO3SpH8d","trusted":true},"outputs":[],"source":["df['New_Glucose_Class'] = pd.cut(\n","    x=df['Glucose'],\n","    bins=[0, 139, 200],\n","    labels=[\"Normal\", \"Pre-Diabetes\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HSX55JjgOfBu"},"outputs":[],"source":["df['New_Glucose_Class'].value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHtG4ylepH8d","trusted":true},"outputs":[],"source":["df['New_BMI_Range'] = pd.cut(\n","    x=df['BMI'],\n","    bins=[0, 18.5, 24.9, 29.9, 100],\n","    labels=[\"Underweight\", \"Healthy\", \"Overweight\", \"Obese\"]\n",")\n","\n","df['New_BMI_Range'].value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Nt6tu5wpH8d","trusted":true},"outputs":[],"source":["df['New_BloodPressure'] = pd.cut(\n","    x=df['BloodPressure'],\n","    bins=[0, 79, 89, 123],\n","    labels=[\"Normal\", \"HS1\", \"HS2\"]\n",")\n","\n","df['New_BloodPressure'].value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3_QLmXmpH8d","trusted":true},"outputs":[],"source":["df['New_SkinThickness'] = (\n","    df['SkinThickness']\n","    .apply(lambda x: 1 if x <= 18.0 else 0)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bHAg0eRpH8j","trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"kKgHHewrvITK"},"source":["## One-Hot Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hK4mk2spH8j","trusted":true},"outputs":[],"source":["def one_hot_encoder(\n","        df, categorical_feats,\n","        nan_as_category=False):\n","\n","    original_columns = list(df.columns)\n","\n","    df = pd.get_dummies(\n","        df,\n","        columns=categorical_feats,\n","        dummy_na=nan_as_category,\n","        drop_first=True\n","    )\n","\n","    new_columns = [col for col in df.columns if col not in original_columns]\n","    return df, new_columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSQEXDTfpH8j","trusted":true},"outputs":[],"source":["categorical_feats = [feat for feat in df.columns if len(df[feat].unique()) <= 10 and feat != \"Outcome\"]\n","categorical_feats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jvf_dRnOpH8j","trusted":true},"outputs":[],"source":["df, new_cols_ohe = one_hot_encoder(df, categorical_feats)\n","new_cols_ohe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIypy3SQpH8j","trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"LMvA-owVvhR4"},"source":["## Feature Scaling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"150aMKB75DBp"},"outputs":[],"source":["like_num = [col for col in df.columns if df[col].dtypes != 'O' and len(df[col].value_counts()) < 10]\n","no_need_to_scaled = new_cols_ohe + [\"Outcome\"] + like_num\n","cols_need_scale = [col for col in df.columns if col not in no_need_to_scaled]\n","\n","print(\"List of columns that need to be scaled:\\n\", cols_need_scale)\n","rs = RobustScaler()\n","df.loc[:, cols_need_scale] = rs.fit_transform(df[cols_need_scale])\n","print(\"Feature Scaling, Done!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QkXRrS5dZVhF"},"outputs":[],"source":["like_num"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GOcFplC-9oy8"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEnh2wYPpH8k","trusted":true},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"aGpBtlaRpH8k"},"source":["# 6.&nbsp;Modeling"]},{"cell_type":"markdown","metadata":{"id":"tbdRBJyC71R6"},"source":["## Notes: Metric Evaluation\n","\n","See [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n","\n","The choice between micro-average, macro-average, or weighted-average in the classification_report from scikit-learn depends on your specific use case and the characteristics of your data. Each average type provides a different perspective on the overall model performance.\n","\n","Here's a brief explanation of each average type:\n","\n","1. **Micro-average:**\n","\n","    Calculates metrics globally by considering all instances together.\n","\n","    Suitable when classes are imbalanced, and you want to treat all instances equally.\n","    Gives equal weight to each data point, regardless of class.\n","\n","2. **Macro-average:**\n","\n","    Calculates metrics for each class independently and then takes the unweighted average.\n","\n","    Suitable when you want to evaluate the overall performance across all classes without considering class imbalances.\n","    Gives equal weight to each class, regardless of the number of instances in each class.\n","\n","3. **Weighted-average:**\n","\n","    Calculates metrics for each class independently and then takes the average, weighted by the number of true instances for each class.\n","\n","    Suitable when classes are imbalanced, and you want to give more importance to the performance on larger classes.\n","    Provides a balanced view of the overall performance by accounting for class imbalances.\n","\n","In summary:\n","\n","- Use micro-average when you want to treat all instances equally, especially in the presence of class imbalances.\n","\n","- Use macro-average when you want to evaluate the overall performance without considering class imbalances.\n","\n","- Use weighted-average when you want to account for class imbalances and give more importance to the larger classes.\n","\n","\n","It's essential to choose the appropriate average based on the goals of your analysis and the nature of your data.\n","\n","In some cases, you may need to consider multiple metrics and averages to get a comprehensive understanding of your model's performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhN1Z7UppH8k","trusted":true},"outputs":[],"source":["X = df.drop(\"Outcome\", axis=1)\n","y = df[\"Outcome\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qEXfKYXwwDvE"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    X, y,\n","    test_size=0.1,    # 10% for testing\n","    stratify=y,       # Stratified sampling based on labels\n","    random_state=42   # Random seed for reproducibility\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NBNGBIQtjSZq"},"outputs":[],"source":["print(\"y train:\", y_train.value_counts(normalize=True))\n","print(\"\\n\")\n","print(\"y test:\", y_test.value_counts(normalize=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2dV_VdCLzz7"},"outputs":[],"source":["from sklearn.metrics import (\n","    accuracy_score, precision_score,\n","    recall_score, f1_score\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sykoKqTWz0Eo"},"outputs":[],"source":["models = [\n","    ('LR', LogisticRegression()),\n","    ('KNN', KNeighborsClassifier()),\n","    ('CART', DecisionTreeClassifier()),\n","    ('RF', RandomForestClassifier()),\n","    ('SVC', SVC(gamma='auto')),\n","    ('XGBM', XGBClassifier()),\n","    ('GB', GradientBoostingClassifier()),\n","    ('LightGB', LGBMClassifier())\n","]\n","\n","# Evaluate each model in turn\n","scorings = [\n","    'accuracy', 'f1_macro',\n","    'precision_macro', 'recall_macro'\n","]\n","\n","model_perf = {}\n","\n","# For each model\n","for name, model in models:\n","\n","    results = {}\n","\n","    # for each scorings\n","    for scoring in scorings:\n","        score_mean = []\n","        score_std = []\n","\n","        # Define K-Fold\n","        kfold = KFold(\n","            n_splits=10, shuffle=True,\n","            random_state=42)\n","\n","        # Training with cross validation\n","        cv_results = cross_val_score(\n","            model, X_train, y_train,\n","            cv=kfold, scoring=scoring)\n","\n","        # Save Training Result\n","        results[scoring] = {\n","            'train_mean': cv_results.mean(),\n","            'train_std': cv_results.std()\n","        }\n","\n","    model_perf[name] = results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEBM6ceVTxX7"},"outputs":[],"source":["model_perf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7Uhwf7yEF3j"},"outputs":[],"source":["focus_metric = 'recall_macro'\n","perf_data = {\n","    \"model_type\": [],\n","    f\"train_avg_{focus_metric}\": [],\n","    f\"train_stddev_{focus_metric}\": []\n","}\n","\n","for model_name, perf in model_perf.items():\n","    mean = perf[focus_metric]['train_mean']\n","    std = perf[focus_metric]['train_std']\n","    perf_data[\"model_type\"].append(model_name)\n","    perf_data[f\"train_avg_{focus_metric}\"].append(mean)\n","    perf_data[f\"train_stddev_{focus_metric}\"].append(std)\n","\n","eval_result = pd.DataFrame(perf_data)\n","eval_result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oYt6YYlAK9iu"},"outputs":[],"source":["model = RandomForestClassifier()\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred_test = model.predict(X_test)\n","\n","# Evaluate the model on the test set\n","accuracy_test = accuracy_score(y_test, y_pred_test)\n","precision_test = precision_score(y_test, y_pred_test, average='macro')\n","recall_test = recall_score(y_test, y_pred_test, average='macro')\n","f1_test = f1_score(y_test, y_pred_test, average='macro')\n","\n","# Print or use the test set scores as needed\n","print(f\"Test Set Accuracy: {accuracy_test:.4f}\")\n","print(f\"Test Set Precision: {precision_test:.4f}\")\n","print(f\"Test Set Recall: {recall_test:.4f}\")\n","print(f\"Test Set F1 Score: {f1_test:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"gsLEmtoxpH8k"},"source":["## 6.1 Model Hyper-Parameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qoajD3lMpH8k","trusted":true},"outputs":[],"source":["# Let's choose the highest 4 models\n","# GBM\n","gbm_model = GradientBoostingClassifier()\n","\n","# Model Tuning\n","gbm_params = {\n","    \"learning_rate\": [0.001, 0.01, 0.1],\n","    \"max_depth\": [3, 5, 8],\n","    \"n_estimators\": [200, 500, 1000],\n","    \"subsample\": [1, 0.5, 0.8]\n","}\n","\n","gbm_cv_model = GridSearchCV(\n","    gbm_model,\n","    gbm_params,\n","    cv=3,\n","    n_jobs=-1,\n","    verbose=2).fit(X, y)\n","\n","print(gbm_cv_model.best_params_)\n","\n","# Final Model\n","gbm_tuned = GradientBoostingClassifier(**gbm_cv_model.best_params_).fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xk_pQNOrpH8k","trusted":true},"outputs":[],"source":["# LightGBM:\n","lgb_model = LGBMClassifier()\n","\n","# Model Tuning\n","lgbm_params = {\n","    \"learning_rate\": [0.01, 0.5, 1],\n","    \"n_estimators\": [200, 500, 1000],\n","    \"max_depth\": [6, 8, 10],\n","    \"colsample_bytree\": [1, 0.4, 0.5]\n","}\n","\n","lgbm_cv_model = GridSearchCV(\n","    lgb_model,\n","    lgbm_params,\n","    cv=3,\n","    n_jobs=-1,\n","    verbose=2).fit(X, y)\n","\n","print(lgbm_cv_model.best_params_)\n","\n","# Final Model\n","lgbm_tuned = LGBMClassifier(**lgbm_cv_model.best_params_).fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsYMreAupH8k","trusted":true},"outputs":[],"source":["# Random Forests:\n","rf_model = RandomForestClassifier()\n","\n","# Model Tuning\n","rf_params = {\n","    \"n_estimators\" :[200, 500, 1000],\n","    \"max_features\": [3, 5, 7],\n","    \"min_samples_split\": [2, 5, 10],\n","    \"max_depth\": [5, 8, None]\n","}\n","\n","rf_cv_model = GridSearchCV(\n","    rf_model,\n","    rf_params,\n","    cv=3,\n","    n_jobs=-1,\n","    verbose=2).fit(X_train, y_train)\n","\n","print(rf_cv_model.best_params_)\n","\n","# Final Model\n","rf_tuned = RandomForestClassifier(**rf_cv_model.best_params_).fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"csphSg5jpH8k","trusted":true},"outputs":[],"source":["# XGB\n","xgb_model = XGBClassifier()\n","\n","# Model Tuning\n","xgb_params = {\n","    \"learning_rate\": [0.01, 0.1, 0.2],\n","    \"min_samples_split\": np.linspace(0.1, 0.5, 10),\n","    \"max_depth\":[3, 5, 8],\n","    \"subsample\":[0.5, 0.9, 1.0],\n","    \"n_estimators\": [100, 1000]\n","}\n","\n","xgb_cv_model = GridSearchCV(\n","    xgb_model,\n","    xgb_params,\n","    cv=3,\n","    n_jobs=-1,\n","    verbose=2).fit(X_train, y_train)\n","\n","print(xgb_cv_model.best_params_)\n","\n","xgb_tuned = XGBClassifier(**xgb_cv_model.best_params_).fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXx4Ft6EpH8k","trusted":true},"outputs":[],"source":["# evaluate each model in turn\n","models = [\n","    ('RF', rf_tuned),\n","    ('GBM', gbm_tuned),\n","    (\"LightGBM\", lgbm_tuned),\n","    (\"XGB\", xgb_tuned),\n","    #...,\n","    #...,\n","]\n","\n","# Evaluate each model in turn\n","scorings = [\n","    'accuracy', 'f1_macro',\n","    'precision_macro', 'recall_macro'\n","]\n","\n","model_training_perf = {}\n","\n","# For each model\n","for name, model in models:\n","\n","    results = {}\n","\n","    # for each scorings\n","    for scoring in scorings:\n","        score_mean = []\n","        score_std = []\n","\n","        # Define K-Fold\n","        kfold = KFold(\n","            n_splits=5, shuffle=True,\n","            random_state=42)\n","\n","        # Training with cross validation\n","        cv_results = cross_val_score(\n","            model, X_test, y_test,\n","            cv=kfold, scoring=scoring)\n","\n","        # Save Training Result\n","        results[scoring] = {\n","            'train_mean': cv_results.mean(),\n","            'train_std': cv_results.std()\n","        }\n","\n","    model_training_perf[name] = results"]},{"cell_type":"markdown","metadata":{"id":"hrJu0GXOArSr"},"source":["## 6.2 Model Training Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6P5VfH41oGtl"},"outputs":[],"source":["import pickle\n","import os\n","os.makedirs(\"models\")\n","\n","# Define the list of models with their names\n","models = [\n","    ('RF', rf_tuned),\n","    ('GBM', gbm_tuned),\n","    (\"LightGBM\", lgbm_tuned),\n","    (\"XGB\", xgb_tuned),\n","]\n","\n","# Iterate over each model in the list\n","for model_name, model in models:\n","    # Specify the file path where you want to save the model\n","    file_path = f\"models/{model_name}_model.pkl\"\n","\n","    # Open the file in binary write mode\n","    with open(file_path, 'wb') as file:\n","        # Use pickle.dump() to serialize and save the model to the file\n","        pickle.dump(model, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3_6TK34weZs"},"outputs":[],"source":["# model_training_perf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2MbFhcgVnoeo"},"outputs":[],"source":["file_path = \"model_training_perf.pkl\"\n","with open(file_path, 'wb') as file:\n","    pickle.dump(model_training_perf, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HbpEWlka6OqV"},"outputs":[],"source":["model_training_perf_df = pd.concat(\n","    {\n","        k: pd.DataFrame.from_dict(v, 'index') for k, v in model_training_perf.items()\n","    },\n","    axis=0\n",")\n","\n","model_training_perf_df.index.rename(\n","    ['model_name', 'eval_metric'],\n","    inplace=True\n",")\n","\n","model_training_perf_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z6jFggmTCT6E"},"outputs":[],"source":["import seaborn as sns\n","sns.set_theme(style=\"whitegrid\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGCZNdwkC2Dy"},"outputs":[],"source":["train_viz_data = (\n","    model_training_perf_df\n","    .loc[(slice(None), slice(None)), :].\n","    train_mean\n","    .reset_index()\n",")\n","\n","train_viz_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2EURAf5xD5P8"},"outputs":[],"source":["# fig, ax = plt.subplots()\n","g=sns.catplot(\n","    data=train_viz_data,\n","    kind=\"bar\",\n","    x=\"model_name\",\n","    y=\"train_mean\",\n","    hue=\"eval_metric\",\n","    palette=\"dark\",\n","    alpha=.6,\n","    height=6\n",");\n","\n","# Set axis labels\n","g.set_axis_labels(\"\", \"mean_score\")\n","\n","# Set legend title\n","g.legend.set_title(\"Metric\")\n","\n","# Set figure title\n","g.fig.suptitle(\"Model Evaluation On Train Data\\n\")\n","\n","# Set y-ticks with a scale of 0.05\n","g.ax.set_yticks([i * 0.05 for i in range(int(g.ax.get_ylim()[1] / 0.05) + 1)])\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"8y3TqVHJA8Uw"},"source":["## 6.3 Model Selection\n","Evaluation on Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rRtdd2xDAhjO"},"outputs":[],"source":["import pickle\n","\n","# Define a list to store the loaded models\n","loaded_models = []\n","\n","# Define the list of model names\n","model_names = ['RF', 'GBM', 'LightGBM', 'XGB']\n","\n","# Iterate over each model name\n","for model_name in model_names:\n","    # Specify the file path of the corresponding .pkl file\n","    file_path = f\"models/{model_name}_model.pkl\"\n","\n","    # Open the file in binary read mode\n","    with open(file_path, 'rb') as file:\n","        # Use pickle.load() to deserialize and load the model from the file\n","        loaded_model = pickle.load(file)\n","\n","        # Append the loaded model to the list of loaded models\n","        loaded_models.append((model_name, loaded_model))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Grrb-pV02Yz"},"outputs":[],"source":["from sklearn.metrics import (\n","    accuracy_score, f1_score,\n","    precision_score, recall_score\n",")\n","\n","# Define lists to store the evaluation metrics for each model\n","accuracies = []\n","f1_scores = []\n","precisions = []\n","recalls = []\n","\n","# Iterate over each loaded model\n","for model_name, loaded_model in loaded_models:\n","    # Predict using the loaded model on the test data\n","    y_pred = loaded_model.predict(X_test)\n","\n","    # Calculate evaluation metrics\n","    accuracy = accuracy_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","\n","    # Append the metrics to the respective lists\n","    accuracies.append((model_name, accuracy))\n","    f1_scores.append((model_name, f1))\n","    precisions.append((model_name, precision))\n","    recalls.append((model_name, recall))\n","\n","evaluation_results = pd.DataFrame({\n","    'Model': [model_name for model_name, _ in loaded_models],\n","    'Accuracy': [accuracy for _, accuracy in accuracies],\n","    'F1 Score': [f1 for _, f1 in f1_scores],\n","    'Precision': [precision for _, precision in precisions],\n","    'Recall': [recall for _, recall in recalls],\n","})\n","\n","# Print the DataFrame\n","evaluation_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4EE9uQjk4Hpl"},"outputs":[],"source":["# Melt the evaluation_results DataFrame\n","melted_evaluation_results = pd.melt(\n","    evaluation_results,\n","    id_vars=['Model'],\n","    value_vars=[\n","        'Accuracy', 'F1 Score',\n","        'Precision', 'Recall'\n","    ],\n","    var_name='Metric',\n","    value_name='Value'\n",")\n","\n","# Print the melted DataFrame\n","melted_evaluation_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JY9nO6Ed02VL"},"outputs":[],"source":["# fig, ax = plt.subplots()\n","g=sns.catplot(\n","    data=melted_evaluation_results,\n","    kind=\"bar\",\n","    x=\"Model\",\n","    y=\"Value\",\n","    hue=\"Metric\",\n","    palette=\"dark\",\n","    alpha=.6,\n","    height=6\n",");\n","\n","# Set axis labels\n","g.set_axis_labels(\"\", \"score\")\n","\n","# Set legend title\n","g.legend.set_title(\"Metric\")\n","\n","# Set figure title\n","g.fig.suptitle(\"Model Evaluation On Test Data\")\n","\n","# Set y-ticks with a scale of 0.05\n","g.ax.set_yticks([i * 0.05 for i in range(int(g.ax.get_ylim()[1] / 0.05) + 1)])\n","\n","# Show the plot\n","plt.show()"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
